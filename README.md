# Make Me Do That
## This project aims to present a simple method for “do as I do” stance transfer. Given a source photograph of a person performing an action, we intent to transfer that action as well as the attire worn by the person to the image of the target. The project consists of two parts - make-me-do-that and clothes-transfer. The former aims to transfer the pose while the later aims to transfer the attire. Results from the first module are fed as input to the second one. Presently the results from each module have been taken separately, as the outputs from the first module weren't good enough to feed the second module. It is a conceptual project with an output accuracy of 70%.
### This project is influenced by the following two repositories  https://github.com/andrewjong/SwapNet, https://github.com/nyoki-mtl/pytorch-EverybodyDanceNow. Many thanks to the authors for their detailed implementations of the original papers which has assisted in bringing the project together.
Contributors: Hashim Salim, David Alapat
